{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Qs05wnIo4NsV"
   },
   "outputs": [],
   "source": [
    "using Pkg\n",
    "using Flux\n",
    "using MLDatasets\n",
    "using Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "fciKXfaQ4NsZ"
   },
   "outputs": [],
   "source": [
    "train_x, train_y = CIFAR10.traindata(Float32, 1:5000)\n",
    "test_x, test_y = CIFAR10.testdata(Float32, 1:5000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ZjwdLawu4Nsa",
    "outputId": "97ce3241-cc46-4663-9ef2-559c0d74dd46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of each image: (32, 32, 3, 5000)\n",
      "Label of 50th training datapoint: 0\n"
     ]
    }
   ],
   "source": [
    "println(\"Size of each image: \", size(train_x))\n",
    "println(\"Label of 50th training datapoint: \", train_y[50])\n",
    "# So here we can see that each training point is a 3D array - a 32x32 image with 3 color channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "XKn7OEQE4Nsb",
    "outputId": "e617b529-e3d9-4562-9549-a9725a8109bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "# Since this is a multi-class classification problem, we can use one hot encoding, just like the MNIST dataset.\n",
    "# There's 10 classes just like mnist, so we encode from 0 to 9\n",
    "\n",
    "train_y, test_y = Flux.onehotbatch(train_y, 0:9), Flux.onehotbatch(test_y, 0:9)\n",
    "nclasses = length(train_y[:,1])\n",
    "println(\"number of classes: \", nclasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "cibc0mO44Nsd",
    "outputId": "d2d3e955-ecd2-4b5c-bb38-8a25e4693f74"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Conv((3, 3), 3 => 32, relu, pad=1),   \u001b[90m# 896 parameters\u001b[39m\n",
       "  Conv((3, 3), 32 => 32, relu, pad=1),  \u001b[90m# 9_248 parameters\u001b[39m\n",
       "  MaxPool((2, 2)),\n",
       "  Dropout(0.2),\n",
       "  Conv((3, 3), 32 => 64, relu, pad=1),  \u001b[90m# 18_496 parameters\u001b[39m\n",
       "  Conv((3, 3), 64 => 64, relu, pad=1),  \u001b[90m# 36_928 parameters\u001b[39m\n",
       "  MaxPool((2, 2)),\n",
       "  Dropout(0.2),\n",
       "  Conv((3, 3), 64 => 128, relu, pad=1),  \u001b[90m# 73_856 parameters\u001b[39m\n",
       "  Conv((3, 3), 128 => 256, relu, pad=1),  \u001b[90m# 295_168 parameters\u001b[39m\n",
       "  MaxPool((2, 2)),\n",
       "  Dropout(0.2),\n",
       "  Flux.flatten,\n",
       "  Dense(4096, 128, relu),               \u001b[90m# 524_416 parameters\u001b[39m\n",
       "  Dropout(0.2),\n",
       "  Dense(128, 10),                       \u001b[90m# 1_290 parameters\u001b[39m\n",
       "  NNlib.softmax,\n",
       ")\u001b[90m                   # Total: 16 arrays, \u001b[39m960_298 parameters, 3.666 MiB."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 VGG Block\n",
    "model_VGG3 = Chain(\n",
    "              Conv((3,3), 3=>32, relu, pad=SamePad()),\n",
    "              Conv((3,3), 32=>32, relu, pad=SamePad()),\n",
    "              MaxPool((2,2)),\n",
    "              Dropout(0.2),\n",
    "              Conv((3,3), 32=>64, relu, pad=SamePad()),\n",
    "              Conv((3,3), 64=>64, relu, pad=SamePad()),\n",
    "              MaxPool((2,2)),\n",
    "              Dropout(0.2),\n",
    "              Conv((3,3), 64=>128, relu, pad=SamePad()),\n",
    "              Conv((3,3), 128=>256, relu, pad=SamePad()),\n",
    "              MaxPool((2,2)),\n",
    "              Dropout(0.2),\n",
    "              Flux.flatten,\n",
    "              Dense(4096,128,relu),\n",
    "              Dropout(0.2),\n",
    "              Dense(128,10),\n",
    "              softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "zfPP93qo4Nsd",
    "outputId": "703629a5-3e9b-48cd-bfa3-0789a25f0878"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss_and_accuracy (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function loss_and_accuracy(udata, wdata, model)\n",
    "\n",
    "    ndata = size(udata,4)\n",
    "\n",
    "    ŵ = model(udata)\n",
    "    loss = Flux.crossentropy(ŵ, wdata; agg=sum)\n",
    "    accuracy = sum(Flux.onecold(ŵ) .== Flux.onecold(wdata)) / ndata\n",
    "    return loss, accuracy\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function loss(udata, wdata, model)\n",
    "\n",
    "    ndata = size(udata,4)\n",
    "\n",
    "    ŵ = model(udata)\n",
    "    loss = Flux.crossentropy(ŵ, wdata; agg=sum)\n",
    "    return loss\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "qMPMOsv44Nse"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_loader = Flux.Data.DataLoader((train_x, train_y), batchsize=batch_size, shuffle=true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "EK3iUdGA4Nsf",
    "outputId": "1b3c262f-a5c9-4286-da32-a226543bb0c9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train(model,train_loader,optimizer,train_x,train_y,test_x,test_y,model_name)\n",
    "    train_losses = []\n",
    "    train_accuracy = []\n",
    "    test_losses = []\n",
    "    test_accuracy = []\n",
    "    α = 0.001 # <- stepsize; in the ML community, it is often denoted as a `learning rate η`\n",
    "    #opt = optimizer(α) \n",
    "    opt = optimizer \n",
    "    K = 1\n",
    "    for k in 1:K\n",
    "        for (u, w) in train_loader\n",
    "            gs = gradient(() -> Flux.Losses.crossentropy(model(u), w), Flux.params(model)) # compute gradient\n",
    "            Flux.Optimise.update!(opt, Flux.params(model), gs) # update parameters\n",
    "        end\n",
    "        println(\"Epoch $k for $model_name architecture.\")\n",
    "        train_loss, train_acc = loss_and_accuracy(train_x, train_y,  model)\n",
    "\n",
    "        test_loss, test_acc = loss_and_accuracy(test_x, test_y, model)\n",
    "\n",
    "        println(\"  train_loss = $train_loss, train_accuracy = $train_acc\")\n",
    "        println(\"  test_loss = $test_loss, test_accuracy = $test_acc\")\n",
    "        \n",
    "        push!(test_losses, test_loss)\n",
    "        push!(test_accuracy, test_acc)\n",
    "        push!(train_losses, train_loss)\n",
    "        push!(train_accuracy, train_acc)\n",
    "    end\n",
    "    return train_losses, train_accuracy, test_losses, test_accuracy\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.researchgate.net/post/How-to-adjust-deep-learning-parameters-using-Particle-swarm-optimization-PSO\n",
    "#= You need to (1) define your fitness function to be the accuracy of the model. \n",
    "(2) initiate a population where each point is a vector containing the number of hidden layers and the \n",
    "number of features per layer (or any other network parameters based on your architecture). \n",
    "For each of these you need to train the model and evaluate the fitness function (calculate accuracy). \n",
    "(3) iterate using position and velocity updates (regular pso) until you satisfy the stopping conditions.\n",
    "\n",
    "I am not entirely sure how efficient this whole process will be though. \n",
    "(1) You still need to set the parametrs of pso itself, and (\n",
    "2) depending on your network architecture, the time it takes to train, and the parameters of that training, \n",
    "it might be very time consuming, and (3) there is no guarantee of converging to an optimal or near-optimal solution. \n",
    "On the bright side, since the fitness function itself is the accuracy,\n",
    "you can always know if the model is doing better.=#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "to_model (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function to_model(vector)\n",
    "    # while of course we could make something fancy out of this, for now \n",
    "    # let's just have it encode back to our original model\n",
    "    # if we were building a library or something we'd store more information about the\n",
    "    # original model to not have to rebuild it like this\n",
    "    return Chain(\n",
    "              Conv((3,3), 3=>32, relu, pad=SamePad(), weight=reshape(vector[1:864],(3,3,3,32)), bias=vector[865:896]),# 896 - 32 bias 864\n",
    "              Conv((3,3), 32=>32, relu, pad=SamePad(),weight=reshape(vector[897:10112],(3,3,32,32)), bias=vector[10112:10143]),# 9248 - 32 bias 9216\n",
    "              MaxPool((2,2)),\n",
    "              Dropout(0.2),\n",
    "              Conv((3,3), 32=>64, relu, pad=SamePad(), weight=reshape(vector[10144:28575], (3,3,32,64)), bias=vector[28575:28638]), # 18496 - 64 = 18432 \n",
    "              Conv((3,3), 64=>64, relu, pad=SamePad(), weight=reshape(vector[28639:65502], (3,3,64,64)), bias=vector[65503:65566]), # 36928 - 64 = 36864\n",
    "              MaxPool((2,2)),\n",
    "              Dropout(0.2),\n",
    "              Conv((3,3), 64=>128, relu, pad=SamePad(),weight=reshape(vector[65568:139295], (3,3,64,128)), bias=vector[139296:139423]), # 73856 - 128 = 73728\n",
    "              Conv((3,3), 128=>256, relu, pad=SamePad(),weight=reshape(vector[139424:434335], (3,3,128,256)), bias=vector[434336:434591]), # 295168 - 256 = 294912\n",
    "              MaxPool((2,2)),\n",
    "              Dropout(0.2),\n",
    "              Flux.flatten,\n",
    "              Dense(reshape(vector[434592:958879],(128,4096)),vector[958880:959007],relu), # 524416 - 128 bias = 524288\n",
    "              Dropout(0.2),\n",
    "              Dense(reshape(vector[959008:960287], (10,128)),vector[960288:960297],relu)) # 1290 - 10 bias = 1280\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct Particle \n",
    "    position::Vector{Float32}\n",
    "    best_position::Vector{Float32}\n",
    "    best_loss::Float32\n",
    "    velocity::Vector{Float32}\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_PSO (generic function with 1 method)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train_PSO(model,train_loader,train_x,train_y,test_x,test_y, numparticles, K)\n",
    "    train_losses = []\n",
    "    train_accuracy = []\n",
    "    test_losses = []\n",
    "    test_accuracy = []\n",
    "    particles = []\n",
    "    best_loss = loss(train_x, train_y,  model)\n",
    "    \n",
    "    params = Flux.params(model)\n",
    "    params_flattened = []\n",
    "    \n",
    "    for p in params\n",
    "        flattened = flatten(p)\n",
    "        final = reshape(flattened, (1, size(flattened, 1) * size(flattened,2)))\n",
    "        append!(params_flattened, final)\n",
    "    end\n",
    "    \n",
    "    swarm_best_position = params_flattened\n",
    "    \n",
    "    #init particles\n",
    "    for n in 1:numparticles\n",
    "        push!(particles, Particle(params_flattened,params_flattened,best_loss,rand(Float32,size(params_flattened))))\n",
    "    end\n",
    "    d = size(params_flattened)\n",
    "    println(\"Init particles\")\n",
    "    \n",
    "    for k in 1:K\n",
    "        for (u, w) in train_loader\n",
    "            train_loss = loss(train_x, train_y,  model)\n",
    "            for p in particles\n",
    "                p.velocity = rand((-1.0,1.0),d) .* p.velocity .+ (p.best_position .- p.position) .+ swarm_best_position .- p.position\n",
    "                #losses are going negative, need to make sure that these values are always between -1 and 1\n",
    "                p.position = p.position .+ p.velocity\n",
    "            \n",
    "                moved_model = to_model(p.position)\n",
    "                this_loss = loss(train_x,train_y, moved_model)\n",
    "                # why is loss negative?\n",
    "                if p.best_loss > this_loss\n",
    "                    p.best_loss = this_loss\n",
    "                    p.best_position = p.position\n",
    "                end\n",
    "                \n",
    "                if best_loss > this_loss\n",
    "                    best_loss = this_loss\n",
    "                    swarm_best_position = p.position\n",
    "                    model = moved_model\n",
    "                    println(\"loss changed: \")\n",
    "                    println(best_loss)\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "\n",
    "        train_loss, train_acc = loss_and_accuracy(train_x, train_y,  model)\n",
    "\n",
    "        test_loss, test_acc = loss_and_accuracy(test_x, test_y, model)\n",
    "\n",
    "        println(\"  train_loss = $train_loss, train_accuracy = $train_acc\")\n",
    "        println(\"  test_loss = $test_loss, test_accuracy = $test_acc\")\n",
    "        \n",
    "        push!(test_losses, test_loss)\n",
    "        push!(test_accuracy, test_acc)\n",
    "        push!(train_losses, train_loss)\n",
    "        push!(train_accuracy, train_acc)\n",
    "    end\n",
    "    return train_losses, train_accuracy, test_losses, test_accuracy, trained_model\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init particles\n",
      "loss changed: \n",
      "-16948.178\n",
      "loss changed: \n",
      "-34652.25\n",
      "loss changed: \n",
      "-43505.97\n",
      "loss changed: \n",
      "-46002.613\n",
      "loss changed: \n",
      "-50752.344\n",
      "loss changed: \n",
      "-58597.17\n",
      "loss changed: \n",
      "-59744.438\n",
      "loss changed: \n",
      "-71237.08\n",
      "loss changed: \n",
      "-118723.445\n",
      "loss changed: \n",
      "-122621.09\n",
      "loss changed: \n",
      "-201062.6\n",
      "loss changed: \n",
      "-220559.83\n",
      "loss changed: \n",
      "-243602.94\n",
      "loss changed: \n",
      "-252246.66\n"
     ]
    }
   ],
   "source": [
    "vgg3_train_loss, \n",
    "vgg3_train_accuracy, \n",
    "vgg3_test_loss, \n",
    "vgg3_test_accuracy,trained_model = train_PSO(model_VGG3, train_loader, train_x, train_y,test_x, test_y, 10,3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "ixnHnYkc4Nsg"
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: train not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: train not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[109]:1",
      " [2] eval",
      "   @ ./boot.jl:360 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1116"
     ]
    }
   ],
   "source": [
    "vgg3_train_loss, \n",
    "vgg3_train_accuracy, \n",
    "vgg3_test_loss, \n",
    "vgg3_test_accuracy = train(model_VGG3, train_loader, ADAM(0.001),train_x, train_y,test_x, test_y, \"VGG3\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "CSE_598_Project_YZ_v2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Julia 1.6.2",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
