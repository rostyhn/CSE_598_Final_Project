{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Qs05wnIo4NsV"
   },
   "outputs": [],
   "source": [
    "using Pkg\n",
    "using Flux\n",
    "using MLDatasets\n",
    "using Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "fciKXfaQ4NsZ"
   },
   "outputs": [],
   "source": [
    "train_x, train_y = CIFAR10.traindata(Float32, 1:5000)\n",
    "test_x, test_y = CIFAR10.testdata(Float32, 1:5000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ZjwdLawu4Nsa",
    "outputId": "97ce3241-cc46-4663-9ef2-559c0d74dd46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of each image: (32, 32, 3, 5000)\n",
      "Label of 50th training datapoint: 0\n"
     ]
    }
   ],
   "source": [
    "println(\"Size of each image: \", size(train_x))\n",
    "println(\"Label of 50th training datapoint: \", train_y[50])\n",
    "# So here we can see that each training point is a 3D array - a 32x32 image with 3 color channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "XKn7OEQE4Nsb",
    "outputId": "e617b529-e3d9-4562-9549-a9725a8109bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "# Since this is a multi-class classification problem, we can use one hot encoding, just like the MNIST dataset.\n",
    "# There's 10 classes just like mnist, so we encode from 0 to 9\n",
    "\n",
    "train_y, test_y = Flux.onehotbatch(train_y, 0:9), Flux.onehotbatch(test_y, 0:9)\n",
    "nclasses = length(train_y[:,1])\n",
    "println(\"number of classes: \", nclasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "cibc0mO44Nsd",
    "outputId": "d2d3e955-ecd2-4b5c-bb38-8a25e4693f74"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Conv((3, 3), 3 => 32, relu, pad=1),   \u001b[90m# 896 parameters\u001b[39m\n",
       "  Conv((3, 3), 32 => 32, relu, pad=1),  \u001b[90m# 9_248 parameters\u001b[39m\n",
       "  MaxPool((2, 2)),\n",
       "  Dropout(0.2),\n",
       "  Conv((3, 3), 32 => 64, relu, pad=1),  \u001b[90m# 18_496 parameters\u001b[39m\n",
       "  Conv((3, 3), 64 => 64, relu, pad=1),  \u001b[90m# 36_928 parameters\u001b[39m\n",
       "  MaxPool((2, 2)),\n",
       "  Dropout(0.2),\n",
       "  Conv((3, 3), 64 => 128, relu, pad=1),  \u001b[90m# 73_856 parameters\u001b[39m\n",
       "  Conv((3, 3), 128 => 256, relu, pad=1),  \u001b[90m# 295_168 parameters\u001b[39m\n",
       "  MaxPool((2, 2)),\n",
       "  Dropout(0.2),\n",
       "  Flux.flatten,\n",
       "  Dense(4096, 128, relu),               \u001b[90m# 524_416 parameters\u001b[39m\n",
       "  Dropout(0.2),\n",
       "  Dense(128, 10),                       \u001b[90m# 1_290 parameters\u001b[39m\n",
       "  NNlib.softmax,\n",
       ")\u001b[90m                   # Total: 16 arrays, \u001b[39m960_298 parameters, 3.666 MiB."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 VGG Block\n",
    "model_VGG3 = Chain(\n",
    "              Conv((3,3), 3=>32, relu, pad=SamePad()),\n",
    "              Conv((3,3), 32=>32, relu, pad=SamePad()),\n",
    "              MaxPool((2,2)),\n",
    "              Dropout(0.2),\n",
    "              Conv((3,3), 32=>64, relu, pad=SamePad()),\n",
    "              Conv((3,3), 64=>64, relu, pad=SamePad()),\n",
    "              MaxPool((2,2)),\n",
    "              Dropout(0.2),\n",
    "              Conv((3,3), 64=>128, relu, pad=SamePad()),\n",
    "              Conv((3,3), 128=>256, relu, pad=SamePad()),\n",
    "              MaxPool((2,2)),\n",
    "              Dropout(0.2),\n",
    "              Flux.flatten,\n",
    "              Dense(4096,128,relu),\n",
    "              Dropout(0.2),\n",
    "              Dense(128,10),\n",
    "              softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "zfPP93qo4Nsd",
    "outputId": "703629a5-3e9b-48cd-bfa3-0789a25f0878"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss_and_accuracy (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function loss_and_accuracy(udata, wdata, model)\n",
    "\n",
    "    ndata = size(udata,4)\n",
    "\n",
    "    ŵ = model(udata)\n",
    "    loss = Flux.crossentropy(ŵ, wdata; agg=sum)\n",
    "    accuracy = sum(Flux.onecold(ŵ) .== Flux.onecold(wdata)) / ndata\n",
    "    return loss, accuracy\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qMPMOsv44Nse"
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "train_loader = Flux.Data.DataLoader((train_x, train_y), batchsize=batch_size, shuffle=true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EK3iUdGA4Nsf",
    "outputId": "1b3c262f-a5c9-4286-da32-a226543bb0c9"
   },
   "outputs": [],
   "source": [
    "function train(model,train_loader,optimizer,train_x,train_y,test_x,test_y,model_name)\n",
    "    train_losses = []\n",
    "    train_accuracy = []\n",
    "    test_losses = []\n",
    "    test_accuracy = []\n",
    "    α = 0.001 # <- stepsize; in the ML community, it is often denoted as a `learning rate η`\n",
    "    #opt = optimizer(α) \n",
    "    opt = optimizer \n",
    "    K = 1\n",
    "    for k in 1:K\n",
    "        for (u, w) in train_loader\n",
    "            gs = gradient(() -> Flux.Losses.crossentropy(model(u), w), Flux.params(model)) # compute gradient\n",
    "            Flux.Optimise.update!(opt, Flux.params(model), gs) # update parameters\n",
    "        end\n",
    "        println(\"Epoch $k for $model_name architecture.\")\n",
    "        train_loss, train_acc = loss_and_accuracy(train_x, train_y,  model)\n",
    "\n",
    "        test_loss, test_acc = loss_and_accuracy(test_x, test_y, model)\n",
    "\n",
    "        println(\"  train_loss = $train_loss, train_accuracy = $train_acc\")\n",
    "        println(\"  test_loss = $test_loss, test_accuracy = $test_acc\")\n",
    "        \n",
    "        push!(test_losses, test_loss)\n",
    "        push!(test_accuracy, test_acc)\n",
    "        push!(train_losses, train_loss)\n",
    "        push!(train_accuracy, train_acc)\n",
    "    end\n",
    "    return train_losses, train_accuracy, test_losses, test_accuracy\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "to_model (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function to_model(vector)\n",
    "    # while of course we could make something fancy out of this, for now \n",
    "    # let's just have it encode back to our original model\n",
    "    # if we were building a library or something we'd store more information about the\n",
    "    # original model to not have to rebuild it like this\n",
    "    return Chain(\n",
    "              Conv((3,3), 3=>32, relu, pad=SamePad(), weight=reshape(vector[1:864],(3,3,3,32)), bias=vector[865:896]),# 896 - 32 bias 864\n",
    "              Conv((3,3), 32=>32, relu, pad=SamePad(),weight=reshape(vector[897:10112],(3,3,32,32)), bias=vector[10112:10143]),# 9248 - 32 bias 9216\n",
    "              MaxPool((2,2)),\n",
    "              Dropout(0.2),\n",
    "              Conv((3,3), 32=>64, relu, pad=SamePad(), weight=reshape(vector[10144:28575], (3,3,32,64)), bias=vector[28575:28638]), # 18496 - 64 = 18432 \n",
    "              Conv((3,3), 64=>64, relu, pad=SamePad(), weight=reshape(vector[28639:65502], (3,3,64,64)), bias=vector[65503:65566]), # 36928 - 64 = 36864\n",
    "              MaxPool((2,2)),\n",
    "              Dropout(0.2),\n",
    "              Conv((3,3), 64=>128, relu, pad=SamePad(),weight=reshape(vector[65568:139295], (3,3,64,128)), bias=vector[139296:139423]), # 73856 - 128 = 73728\n",
    "              Conv((3,3), 128=>256, relu, pad=SamePad(),weight=reshape(vector[139424:434335], (3,3,128,256)), bias=vector[434336:434591]), # 295168 - 256 = 294912\n",
    "              MaxPool((2,2)),\n",
    "              Dropout(0.2),\n",
    "              Flux.flatten,\n",
    "              Dense(reshape(vector[434592:958879],(128,4096)),vector[958880:959007],relu), # 524416 - 128 bias = 524288\n",
    "              Dropout(0.2),\n",
    "              Dense(reshape(vector[959008:960287], (10,128)),vector[960288:960297],relu)) # 1290 - 10 bias = 1280\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct Particle \n",
    "    position::Vector{Float32}\n",
    "    best_position::Vector{Float32}\n",
    "    best_accuracy::Float32\n",
    "    velocity::Vector{Float32}\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_PSO (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train_PSO(model,train_loader,train_x,train_y,test_x,test_y, numparticles, ω, c1,c2)\n",
    "    train_losses = []\n",
    "    train_accuracy = []\n",
    "    test_losses = []\n",
    "    test_accuracy = []\n",
    "    particles = []\n",
    "    best_loss, best_accuracy = loss_and_accuracy(train_x, train_y,  model)\n",
    "    \n",
    "    params = Flux.params(model)\n",
    "    params_flattened = []\n",
    "    \n",
    "    for p in params\n",
    "        flattened = flatten(p)\n",
    "        final = reshape(flattened, (1, size(flattened, 1) * size(flattened,2)))\n",
    "        append!(params_flattened, final)\n",
    "    end\n",
    "    \n",
    "    swarm_best_position = params_flattened\n",
    "    d = size(params_flattened)\n",
    "    \n",
    "    #init particles\n",
    "    for n in 1:numparticles\n",
    "        push!(particles, Particle(params_flattened,params_flattened,best_accuracy,rand(Float32,size(params_flattened))))\n",
    "    end\n",
    "\n",
    "    println(\"Initalized particles!\")\n",
    "    \n",
    "    for k in 1:3\n",
    "        for (u, w) in train_loader\n",
    "            for p in particles\n",
    "                # vector math for additional speed boost\n",
    "                # ω - inertia weight, how much the previous velocity impacts the current position\n",
    "                # c1 - how much the particle pays attention to its own best position\n",
    "                # c2 - how much the particle pays attention to the swarm's best position\n",
    "                p.velocity = (ω .* p.velocity) .+\n",
    "                (rand((-1.0,1.0),d) * c1 .* (p.best_position .- p.position)) .+\n",
    "                (rand((-1.0,1.0),d) * c2 .* (swarm_best_position .- p.position))\n",
    "                # need to make sure that these values are always between -1.0 and 1\n",
    "                # clamp. broadcasts to entire vector\n",
    "                p.position = clamp.(p.position .+ p.velocity,0.0,1.0)\n",
    "            \n",
    "                moved_model = to_model(p.position)\n",
    "                this_loss, this_acc = loss_and_accuracy(train_x,train_y, moved_model)\n",
    "                \n",
    "                if p.best_accuracy < this_acc\n",
    "                    p.best_accuracy = this_acc\n",
    "                    p.best_position = p.position\n",
    "                end\n",
    "                \n",
    "                if best_accuracy < this_acc\n",
    "                    best_accuracy = this_acc\n",
    "                    swarm_best_position = p.position\n",
    "                    model = moved_model\n",
    "                    println(\"New accuracy: $this_acc\")\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "\n",
    "        train_loss, train_acc = loss_and_accuracy(train_x, train_y,  model)\n",
    "        test_loss, test_acc = loss_and_accuracy(test_x, test_y, model)\n",
    "\n",
    "        println(\"  train_loss = $train_loss, train_accuracy = $train_acc\")\n",
    "        println(\"  test_loss = $test_loss, test_accuracy = $test_acc\")\n",
    "        \n",
    "        push!(test_losses, test_loss)\n",
    "        push!(test_accuracy, test_acc)\n",
    "        push!(train_losses, train_loss)\n",
    "        push!(train_accuracy, train_acc)\n",
    "    end\n",
    "    return train_losses, train_accuracy, test_losses, test_accuracy, model\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: model_VGG3 not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: model_VGG3 not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[15]:1",
      " [2] eval",
      "   @ ./boot.jl:360 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1116"
     ]
    }
   ],
   "source": [
    "vgg3_train_loss, \n",
    "vgg3_train_accuracy, \n",
    "vgg3_test_loss, \n",
    "vgg3_test_accuracy,trained_model = train_PSO(model_VGG3, train_loader, train_x, train_y,test_x, test_y, 10, 0.5, 0.3, 0.4);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ixnHnYkc4Nsg"
   },
   "outputs": [],
   "source": [
    "vgg3_train_loss, \n",
    "vgg3_train_accuracy, \n",
    "vgg3_test_loss, \n",
    "vgg3_test_accuracy = train(model_VGG3, train_loader, ADAM(0.001),train_x, train_y,test_x, test_y, \"VGG3\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: \n",
    "# Generalize to_model function (difficult, not necessary)\n",
    "# Run model with several different optimizer with different hyper parameters\n",
    "# Run PSO with different hyper-parameters\n",
    "# Build table with accuracy, loss plotted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization with PSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct Particle \n",
    "    position::Vector{Int}\n",
    "    best_position::Vector{Int}\n",
    "    best_accuracy::Float32\n",
    "    velocity::Vector{Float32}\n",
    "end\n",
    "# try sizes 4, 10, 16 - even small swarms are good\n",
    "# Conv SF = kernel size;  n = number of outputs\n",
    "# Pooling SP - kernel size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hyper_parameterized (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function hyper_parameterized(particle)\n",
    "    return Chain(\n",
    "              Conv((particle.position[1],particle.position[1]), 3=>particle.position[2], relu, pad=SamePad()),\n",
    "              Conv((particle.position[3],particle.position[3]), particle.position[2]=>particle.position[4], relu, pad=SamePad()),\n",
    "              MaxPool((particle.position[5],particle.position[5])),\n",
    "              Dropout(0.2),\n",
    "              Conv((particle.position[6],particle.position[6]), particle.position[4]=>particle.position[7], relu, pad=SamePad()),\n",
    "              Conv((particle.position[8],particle.position[8]), particle.position[7]=>particle.position[9], relu, pad=SamePad()),\n",
    "              MaxPool((particle.position[10],particle.position[10])),\n",
    "              Dropout(0.2),\n",
    "              Conv((particle.position[11],particle.position[11]), particle.position[9]=>particle.position[12], relu, pad=SamePad()),\n",
    "              Conv((particle.position[13],particle.position[14]), particle.position[12]=>particle.position[14], relu, pad=SamePad()),\n",
    "              MaxPool((particle.position[15],particle.position[15])),\n",
    "              Dropout(0.2),\n",
    "              Flux.flatten,\n",
    "              Dense(particle.position[15]^4 * particle.position[14],particle.position[16],relu),\n",
    "              Dropout(0.2),\n",
    "              Dense(particle.position[16],10),\n",
    "              softmax)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hyper_parameter_optimization (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function hyper_parameter_optimization(train_x, train_y, test_x, test_y, numparticles, ω, c1,c2)\n",
    "    lo = [2,1,2,1,2,2,1,2,1,2,2,2,2,2,2,10]\n",
    "    hi = [10,48,10,100,4,10,100,10,100,4,10,256,10,256,4,1000]\n",
    "    particles = []\n",
    "    best_accuracy = 0\n",
    "    # init particles - search space is 16 variables\n",
    "        #0   #1  #2   #3  #4   #5   #6  #7   #8  #9  #10  #11  #12  #13 #14  #15\n",
    "    # [SF1, n1, SF2, n2, SP1, SF3, n3, SF4, n4, SP2, SF5, n5, SF6, n6, SP3, SD1]\n",
    "    for i in 0:numparticles\n",
    "        SF1 = rand(2:10)\n",
    "        n1 = rand(1:48)\n",
    "        SF2 = rand(2:10)\n",
    "        n2 = rand(n1:100)\n",
    "        SP1 = rand(2:4)\n",
    "        SF3 = rand(2:10)\n",
    "        n3 = rand(n2:100)\n",
    "        SF4 = rand(2:10)\n",
    "        n4 = rand(n3:100)\n",
    "        SP2 = rand(2:4)\n",
    "        SF5 = rand(2:10)\n",
    "        n5 = rand(n4:256)\n",
    "        SF6 = rand(2:10)\n",
    "        n6 = rand(n5:256)\n",
    "        SP3 = rand(2:4)\n",
    "        SD1 = rand(10:1000)\n",
    "        \n",
    "        push!(particles, Particle([SF1,n1,SF2,n2,SP1,SF3,n3,SF4,n4,SP2,SF5,n5,SF6,n6,SP3,SD1],\n",
    "                [SF1,n1,SF2,n2,SP1,SF3,n3,SF4,n4,SP2,SF5,n5,SF6,n6,SP3,SD1], 0, rand(Float32,16)))\n",
    "    end\n",
    "    # find best model to work towards\n",
    "    for p in particles\n",
    "        m = hyper_parameterized(p)\n",
    "        this_loss, this_acc = loss_and_accuracy(train_x,train_y, m)\n",
    "        p.best_accuracy = this_acc\n",
    "        if p.best_accuracy > best_accuracy\n",
    "            swarm_best_position = p.position\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    for p in particles\n",
    "        # vector math for additional speed boost\n",
    "        # ω - inertia weight, how much the previous velocity impacts the current position\n",
    "        # c1 - how much the particle pays attention to its own best position\n",
    "        # c2 - how much the particle pays attention to the swarm's best position\n",
    "        p.velocity = (ω .* p.velocity) .+ (rand((-1.0:1.0)) * c1 .* (p.best_position .- p.position)) .+\n",
    "                (rand((-1.0:1.0)) * c2 .* (swarm_best_position .- p.position))\n",
    "        p.position = clamp.(trunc.(Int, p.position + p.velocity),1,1000)\n",
    "        model = hyper_parameterized(p)\n",
    "        this_loss, this_acc = loss_and_accuracy(train_x,train_y, model)\n",
    "                \n",
    "        if p.best_accuracy < this_acc\n",
    "            p.best_accuracy = this_acc\n",
    "            p.best_position = p.position\n",
    "        end\n",
    "                \n",
    "        if best_accuracy < this_acc\n",
    "            best_accuracy = this_acc\n",
    "            swarm_best_position = p.position\n",
    "            println(best_accuracy)\n",
    "        end\n",
    "        \n",
    "        println(swarm_best_position)\n",
    "    end\n",
    "    return swarm_best_position\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_parameter_optimization(train_x, train_y,test_x, test_y, 10, 0.5, 0.3, 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand(1:10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "CSE_598_Project_YZ_v2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Julia 1.6.2",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
