{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "using Flux\n",
    "using MLDatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = CIFAR10.traindata(Float32, 1:10000)\n",
    "test_x, test_y = CIFAR10.testdata(Float32, 1:10000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of each image: (32, 32, 3, 10000)\n",
      "Label of 50th training datapoint: 0\n"
     ]
    }
   ],
   "source": [
    "println(\"Size of each image: \", size(train_x))\n",
    "println(\"Label of 50th training datapoint: \", train_y[50])\n",
    "# So here we can see that each training point is a 3D array - a 32x32 image with 3 color channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "# Since this is a multi-class classification problem, we can use one hot encoding, just like the MNIST dataset.\n",
    "# There's 10 classes just like mnist, so we encode from 0 to 9\n",
    "\n",
    "train_y, test_y = Flux.onehotbatch(train_y, 0:9), Flux.onehotbatch(test_y, 0:9)\n",
    "nclasses = length(train_y[:,1])\n",
    "println(\"number of classes: \", nclasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Conv((3, 3), 3 => 32, relu),          \u001b[90m# 896 parameters\u001b[39m\n",
       "  Conv((3, 3), 32 => 32, relu),         \u001b[90m# 9_248 parameters\u001b[39m\n",
       "  MaxPool((2, 2)),\n",
       "  Flux.flatten,\n",
       "  Dense(6272, 128),                     \u001b[90m# 802_944 parameters\u001b[39m\n",
       "  Dense(128, 10),                       \u001b[90m# 1_290 parameters\u001b[39m\n",
       "  NNlib.softmax,\n",
       ")\u001b[90m                   # Total: 8 arrays, \u001b[39m814_378 parameters, 3.108 MiB."
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 VGG Block\n",
    "model_VGG1 = Chain(Conv((3,3), 3=>32, relu),\n",
    "              Conv((3,3), 32=>32, relu),\n",
    "              MaxPool((2,2)),\n",
    "              Flux.flatten,\n",
    "              Dense(6272, 128),\n",
    "              Dense(128,10),\n",
    "              softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Conv((3, 3), 3 => 32, relu),          \u001b[90m# 896 parameters\u001b[39m\n",
       "  Conv((3, 3), 32 => 32, relu),         \u001b[90m# 9_248 parameters\u001b[39m\n",
       "  MaxPool((2, 2)),\n",
       "  Conv((3, 3), 32 => 64, relu),         \u001b[90m# 18_496 parameters\u001b[39m\n",
       "  Conv((3, 3), 64 => 64, relu),         \u001b[90m# 36_928 parameters\u001b[39m\n",
       "  MaxPool((2, 2)),\n",
       "  Flux.flatten,\n",
       "  Dense(1600, 128),                     \u001b[90m# 204_928 parameters\u001b[39m\n",
       "  Dense(128, 10),                       \u001b[90m# 1_290 parameters\u001b[39m\n",
       "  NNlib.softmax,\n",
       ")\u001b[90m                   # Total: 12 arrays, \u001b[39m271_786 parameters, 1.039 MiB."
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 VGG Block\n",
    "model_VGG2 = Chain(Conv((3,3), 3=>32, relu),\n",
    "              Conv((3,3), 32=>32, relu),\n",
    "              MaxPool((2,2)),\n",
    "              Conv((3,3), 32=>64, relu),\n",
    "              Conv((3,3), 64=>64, relu),\n",
    "              MaxPool((2,2)),\n",
    "              Flux.flatten,\n",
    "              Dense(1600, 128),\n",
    "              Dense(128,10),\n",
    "              softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×10000 Matrix{Float32}:\n",
       " 0.0979358  0.0973411  0.101644   …  0.0985097  0.0961972  0.0992779\n",
       " 0.0933131  0.0927926  0.0928897     0.0950712  0.0955589  0.0937156\n",
       " 0.0959559  0.0934367  0.0934987     0.0937468  0.0968018  0.0946712\n",
       " 0.0989377  0.0990782  0.100036      0.100753   0.099971   0.100841\n",
       " 0.103142   0.105351   0.103573      0.102786   0.10369    0.104147\n",
       " 0.101781   0.0997584  0.0994495  …  0.099266   0.10094    0.0999218\n",
       " 0.101138   0.10113    0.101842      0.102004   0.101279   0.100691\n",
       " 0.105909   0.105327   0.101732      0.102817   0.104416   0.104005\n",
       " 0.101249   0.10156    0.103003      0.101102   0.0993258  0.102143\n",
       " 0.100639   0.104225   0.102333      0.103945   0.101821   0.100586"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 VGG Block\n",
    "model_VGG3 = Chain(\n",
    "              Conv((3,3), 3=>32, relu),\n",
    "              Conv((3,3), 32=>32, relu),\n",
    "              MaxPool((2,2)),\n",
    "              Conv((3,3), 32=>64, relu),\n",
    "              Conv((3,3), 64=>64, relu),\n",
    "              MaxPool((2,2)),\n",
    "              Conv((3,3), 64=>128, relu),\n",
    "              Conv((3,3), 128=>128, relu),\n",
    "              #MaxPool((2,2)),\n",
    "              Flux.flatten,\n",
    "              Dense(128, 128),\n",
    "              Dense(128,10),\n",
    "              softmax)\n",
    "model_VGG3(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss = 23043.992, train_accuracy = 0.1037\n",
      "test_loss = 23038.322, test_accuracy = 0.1005\n"
     ]
    }
   ],
   "source": [
    "function loss_and_accuracy(udata, wdata, model)\n",
    "\n",
    "    ndata = size(udata,4)\n",
    "\n",
    "    ŵ = model(udata)\n",
    "    loss = Flux.crossentropy(ŵ, wdata; agg=sum)\n",
    "    accuracy = sum(Flux.onecold(ŵ) .== Flux.onecold(wdata)) / ndata\n",
    "    return loss, accuracy\n",
    "end\n",
    "\n",
    "train_loss, train_acc = loss_and_accuracy(train_x,train_y,model_VGG2)\n",
    "test_loss, test_acc = loss_and_accuracy(test_x,test_y,model_VGG2)\n",
    "println(\"train_loss = $train_loss, train_accuracy = $train_acc\")\n",
    "println(\"test_loss = $test_loss, test_accuracy = $test_acc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_loader = Flux.Data.DataLoader((train_x, train_y), batchsize=batch_size, shuffle=true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train (generic function with 1 method)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train(model, train_loader, optimizer, model_params,train_x,train_y,test_x,test_y,model_name)\n",
    "    train_losses = []\n",
    "    train_accuracy = []\n",
    "    test_losses = []\n",
    "    test_accuracy = []\n",
    "    α = 0.001 # <- stepsize; in the ML community, it is often denoted as a `learning rate η`\n",
    "    opt = optimizer(α) \n",
    "    K = 10    # <- Epoch limit; in the ML community, one full iteration over all sub loss functions is often referred to as `epoch`\n",
    "    for k in 1:K\n",
    "        for (u, w) in train_loader\n",
    "            gs = gradient(() -> Flux.Losses.crossentropy(model(u), w), model_params) # compute gradient\n",
    "            Flux.Optimise.update!(opt, model_params, gs) # update parameters\n",
    "        end\n",
    "        println(\"Epoch $k for $model_name architecture.\")\n",
    "        train_loss, train_acc = loss_and_accuracy(train_x, train_y,  model)\n",
    "        push!(train_losses, train_loss)\n",
    "        push!(train_accuracy, train_acc)\n",
    "        test_loss, test_acc = loss_and_accuracy(test_x, test_y, model)\n",
    "        push!(test_losses, test_loss)\n",
    "        push!(test_accuracy, test_acc)\n",
    "        println(\"  train_loss = $train_loss, train_accuracy = $train_acc\")\n",
    "        println(\"  test_loss = $test_loss, test_accuracy = $test_acc\")\n",
    "    end\n",
    "    return train_losses, train_accuracy, test_losses, test_accuracy\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 for VGG1 architecture.\n",
      "  train_loss = 14975.256, train_accuracy = 0.4731\n",
      "  test_loss = 15787.694, test_accuracy = 0.437\n",
      "Epoch 2 for VGG1 architecture.\n",
      "  train_loss = 12392.323, train_accuracy = 0.5601\n",
      "  test_loss = 14063.291, test_accuracy = 0.4936\n",
      "Epoch 3 for VGG1 architecture.\n",
      "  train_loss = 10892.171, train_accuracy = 0.6234\n",
      "  test_loss = 13263.6, test_accuracy = 0.5341\n",
      "Epoch 4 for VGG1 architecture.\n",
      "  train_loss = 9753.802, train_accuracy = 0.6541\n",
      "  test_loss = 13111.36, test_accuracy = 0.5349\n",
      "Epoch 5 for VGG1 architecture.\n",
      "  train_loss = 9238.127, train_accuracy = 0.6749\n",
      "  test_loss = 13269.293, test_accuracy = 0.5463\n",
      "Epoch 6 for VGG1 architecture.\n",
      "  train_loss = 8078.162, train_accuracy = 0.721\n",
      "  test_loss = 13089.453, test_accuracy = 0.5617\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "InterruptException:",
     "output_type": "error",
     "traceback": [
      "InterruptException:",
      "",
      "Stacktrace:",
      "  [1] Pullback",
      "    @ ~/.julia/packages/Flux/ZnXxS/src/layers/basic.jl:47 [inlined]",
      "  [2] (::typeof(∂(applychain)))(Δ::Matrix{Float32})",
      "    @ Zygote ~/.julia/packages/Zygote/ajuwN/src/compiler/interface2.jl:0",
      "  [3] Pullback",
      "    @ ~/.julia/packages/Flux/ZnXxS/src/layers/basic.jl:47 [inlined]",
      "  [4] (::typeof(∂(applychain)))(Δ::Matrix{Float32})",
      "    @ Zygote ~/.julia/packages/Zygote/ajuwN/src/compiler/interface2.jl:0",
      "  [5] Pullback",
      "    @ ~/.julia/packages/Flux/ZnXxS/src/layers/basic.jl:47 [inlined]",
      "  [6] (::typeof(∂(applychain)))(Δ::Matrix{Float32})",
      "    @ Zygote ~/.julia/packages/Zygote/ajuwN/src/compiler/interface2.jl:0",
      "  [7] Pullback",
      "    @ ~/.julia/packages/Flux/ZnXxS/src/layers/basic.jl:49 [inlined]",
      "  [8] (::typeof(∂(λ)))(Δ::Matrix{Float32})",
      "    @ Zygote ~/.julia/packages/Zygote/ajuwN/src/compiler/interface2.jl:0",
      "  [9] Pullback",
      "    @ ./In[76]:11 [inlined]",
      " [10] (::typeof(∂(λ)))(Δ::Float32)",
      "    @ Zygote ~/.julia/packages/Zygote/ajuwN/src/compiler/interface2.jl:0",
      " [11] (::Zygote.var\"#94#95\"{Zygote.Params, typeof(∂(λ)), Zygote.Context})(Δ::Float32)",
      "    @ Zygote ~/.julia/packages/Zygote/ajuwN/src/compiler/interface.jl:348",
      " [12] gradient(f::Function, args::Zygote.Params)",
      "    @ Zygote ~/.julia/packages/Zygote/ajuwN/src/compiler/interface.jl:76",
      " [13] train(model::Chain{Tuple{Conv{2, 4, typeof(relu), Array{Float32, 4}, Vector{Float32}}, Conv{2, 4, typeof(relu), Array{Float32, 4}, Vector{Float32}}, MaxPool{2, 4}, typeof(flatten), Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, typeof(softmax)}}, train_loader::Flux.Data.DataLoader{Tuple{Array{Float32, 4}, Flux.OneHotArray{UInt32, 10, 1, 2, Vector{UInt32}}}, Random._GLOBAL_RNG}, optimizer::Type{ADAM}, model_params::Zygote.Params, train_x::Array{Float32, 4}, train_y::Flux.OneHotArray{UInt32, 10, 1, 2, Vector{UInt32}}, test_x::Array{Float32, 4}, test_y::Flux.OneHotArray{UInt32, 10, 1, 2, Vector{UInt32}}, model_name::String)",
      "    @ Main ./In[76]:11",
      " [14] top-level scope",
      "    @ In[77]:1",
      " [15] eval",
      "    @ ./boot.jl:360 [inlined]",
      " [16] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "    @ Base ./loading.jl:1116"
     ]
    }
   ],
   "source": [
    "vgg1_train_loss, \n",
    "vgg1_train_accuracy, \n",
    "vgg1_test_loss, \n",
    "vgg1_test_accuracy = train(model_VGG1, train_loader, ADAM, Flux.params(model_VGG1), \n",
    "    train_x, train_y,test_x, test_y, \"VGG1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg2_train_loss, \n",
    "vgg2_train_accuracy, \n",
    "vgg2_test_loss, \n",
    "vgg2_test_accuracy = train(model_VGG2, train_loader, ADAM ,Flux.params(model_VGG2), train_x, train_y, test_x,test_y, \"VGG2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg3_train_loss, \n",
    "vgg3_train_accuracy, \n",
    "vgg3_test_loss, \n",
    "vgg3_test_accuracy = train(model_VGG3, train_loader, ADAM ,Flux.params(model_VGG3), train_x, train_y, test_x,test_y,\"VGG3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.2",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
